# Default Training Configuration
# @package _global_
name: default

# Training hyperparameters
batch_size: 256
learning_rate: 0.01
weight_decay: 1e-4
epochs: 20
patience: 5
min_delta: 1e-4

# Note: Optimizer settings are now in separate optimizer configs
# Use: python train_hydra.py optimizer=adam_variant

# Scheduler settings
scheduler:
  type: "reduce_on_plateau"
  factor: 0.5
  patience: 3
  min_lr: 1e-6
  verbose: true

# Loss function settings
loss:
  type: "bce"
  margin: 1.0
  reduction: "mean"

# Validation settings
validation:
  val_ratio: 0.2
  val_batch_size: 512
  val_frequency: 1
  early_stopping: true

# Evaluation metrics
metrics: ["rmse", "mae", "precision@10", "recall@10", "ndcg@10"]
top_k: [5, 10, 20]
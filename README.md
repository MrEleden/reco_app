# ğŸ¬ Mov| Technology | Purpose | Integration Point |
|------------|---------|-------------------|
| **ğŸ”¥ PyTorch** | Deep Learning Framework | Model architecture, training loops, GPU acceleration |
| **âš™ï¸ Hydra** | Configuration Management | Experiment configs, multirun sweeps, parameter overrides |
| **ğŸ”¬ MLflow** | Experiment Tracking | Automatic logging, model registry, performance comparison |
| **ğŸ¯ Optuna** | Hyperparameter Optimization | Intelligent parameter search with Hydra integration |
| **ğŸ Python OOP** | Code Architecture | Modular design, abstract classes, professional structure |
| **ğŸ“Š Scientific Stack** | Data Processing | NumPy, Pandas for data manipulation and analysis |

### **ğŸš€ Key Integration Features**
- **One Command, Full Pipeline**: `python train_hydra.py -m model=collaborative,hybrid,deep_cf`
- **Intelligent Optimization**: `python train_hydra.py --config-name=optuna_test -m` 
- **Automatic Tracking**: Every experiment logged to MLflow with zero extra code
- **Configuration Magic**: Change models, optimizers, hyperparameters via YAML configs
- **Smart HPO**: Optuna finds optimal hyperparameters automatically
- **Production Ready**: Professional error handling, logging, and model persistencetion System Tech Stack Integration

A **production-ready movie recommendation system** demonstrating how modern ML/AI technologies work together seamlessly. This project showcases the integration of **PyTorch**, **Hydra**, **MLflow**, and professional software architecture.

## ğŸ¯ **Tech Stack Showcase**

This project demonstrates **complete integration** of modern ML technologies:

| Technology | Purpose | Integration Point |
|------------|---------|-------------------|
| **ğŸ”¥ PyTorch** | Deep Learning Framework | Model architecture, training loops, GPU acceleration |
| **âš™ï¸ Hydra** | Configuration Management | Experiment configs, multirun sweeps, parameter overrides |
| **ï¿½ MLflow** | Experiment Tracking | Automatic logging, model registry, performance comparison |
| **ï¿½ Python OOP** | Code Architecture | Modular design, abstract classes, professional structure |
| **ğŸ“Š Scientific Stack** | Data Processing | NumPy, Pandas for data manipulation and analysis |

### **ğŸš€ Key Integration Features**
- **One Command, Full Pipeline**: `python train_hydra.py -m model=collaborative,hybrid,deep_cf`
- **Automatic Tracking**: Every experiment logged to MLflow with zero extra code
- **Configuration Magic**: Change models, optimizers, hyperparameters via YAML configs
- **Production Ready**: Professional error handling, logging, and model persistence

## ğŸ† **Live Performance Dashboard**

**Current Best Model**: Hybrid Architecture with **RMSE: 0.3239** and **85.21% Accuracy**

| Model | RMSE â¬‡ï¸ | Accuracy â¬†ï¸ | Status |
|-------|---------|-------------|---------|
| ğŸ¥‡ **Hybrid** | **0.3239** | **85.21%** | âœ… Production Ready |
| ğŸ¥ˆ Collaborative | 0.3451 | 83.65% | âœ… Baseline |
| ğŸ¥‰ Content-Based | 0.3820 | 82.25% | âœ… Specialized |

*Real-time results from MLflow tracking - View full dashboard: http://127.0.0.1:5000*

## ğŸš€ **Quick Start - Tech Stack in Action**

### **1ï¸âƒ£ Install & Setup**
```bash
git clone https://github.com/MrEleden/reco_app.git
cd reco_app
pip install -r requirements.txt
```

### **2ï¸âƒ£ Experience the Integration** 
```bash
# ğŸ¯ Single Model Training (PyTorch + Hydra + MLflow)
python train_hydra.py model=hybrid train.epochs=10

# ğŸ”„ Multi-Model Comparison (Automated Experiment Tracking)
python train_hydra.py -m model=collaborative,hybrid,content_based

# ğŸ“Š View Results Dashboard
python check_mlflow.py
python -m mlflow ui --port 5000  # http://127.0.0.1:5000
```

### **3ï¸âƒ£ Configuration Magic - No Code Changes Needed**

```bash
# ğŸ”§ Change Models via Config
python train_hydra.py model=deep_cf          # Switch to Deep Learning model
python train_hydra.py model=hybrid           # Switch to Hybrid model

# âš¡ Change Training Settings 
python train_hydra.py train=fast             # Quick 10-epoch training
python train_hydra.py train=production       # Full 30-epoch training

# ğŸ›ï¸ Override Any Parameter
python train_hydra.py model=hybrid train.learning_rate=0.005 train.batch_size=512

# ğŸ”„ Hyperparameter Sweeps
python train_hydra.py -m train.learning_rate=0.001,0.01,0.1 train.batch_size=256,512
```

### **4ï¸âƒ£ Intelligent Hyperparameter Optimization**
```bash
# ğŸ¯ Optuna-powered automatic hyperparameter search
python train_hydra.py --config-name=optuna_test -m

# ğŸ”„ Compare: Manual grid search vs Optuna optimization
python train_hydra.py -m train.learning_rate=0.001,0.01 train.batch_size=256,512  # Grid search
python train_hydra.py --config-name=optuna_test -m                                 # Smart search
```

### **5ï¸âƒ£ Production Model Loading**
```python
# ğŸ† Auto-load best performing model
from utils.mlflow_utils import MLflowModelSelector

selector = MLflowModelSelector()
best_model, run_id = selector.load_best_model('val_rmse')
print(f"âœ… Loaded best model: {run_id}")
```

## ğŸ“ **Tech Stack Architecture**

**Clean, Professional Structure Showcasing Modern ML Engineering**

```
reco_app/
â”œâ”€â”€ ğŸš€ train_hydra.py                 # Main entry point - Hydra + PyTorch + MLflow
â”œâ”€â”€ ğŸ”¬ check_mlflow.py                # MLflow results dashboard
â”œâ”€â”€ ğŸ“‹ requirements.txt               # Tech stack dependencies
â”‚
â”œâ”€â”€ âš™ï¸ conf/                           # ğŸ¯ HYDRA Configuration Hub
â”‚   â”œâ”€â”€ config.yaml                   # Main orchestration config
â”‚   â”œâ”€â”€ model/                        # ğŸ§  Model architecture configs
â”‚   â”‚   â”œâ”€â”€ collaborative.yaml        # Matrix factorization
â”‚   â”‚   â”œâ”€â”€ hybrid.yaml               # Multi-modal approach
â”‚   â”‚   â””â”€â”€ deep_cf.yaml              # Deep learning model
â”‚   â”œâ”€â”€ train/                        # ğŸš€ Training configurations
â”‚   â”‚   â”œâ”€â”€ fast.yaml                 # Quick experiments (10 epochs)
â”‚   â”‚   â””â”€â”€ production.yaml           # Full training (30 epochs)
â”‚   â””â”€â”€ optimizer/                    # âš¡ Optimizer settings
â”‚       â”œâ”€â”€ adam.yaml                 # Adaptive learning
â”‚       â””â”€â”€ sgd.yaml                  # Stochastic gradient descent
â”‚
â”œâ”€â”€ ğŸ§  models/                         # ğŸ”¥ PYTORCH Model Architectures
â”‚   â”œâ”€â”€ collaborative_filtering.py    # Matrix factorization
â”‚   â”œâ”€â”€ content_based_model.py        # Genre-based recommendations  
â”‚   â”œâ”€â”€ hybrid_model.py               # Combined approach
â”‚   â””â”€â”€ deep_collaborative_filtering.py # Deep neural networks
â”‚
â”œâ”€â”€ ğŸ“Š data/                           # Data processing pipeline
â”‚   â”œâ”€â”€ dataset.py                    # PyTorch datasets
â”‚   â””â”€â”€ dataloader.py                 # MovieLens data loader
â”‚
â”œâ”€â”€ ğŸ¯ losses/ & ğŸ“ˆ metrics/           # Training components
â”‚   â”œâ”€â”€ loss.py                       # BCE, MSE, ranking losses
â”‚   â””â”€â”€ metric.py                     # RMSE, MAE, Precision@K
â”‚
â”œâ”€â”€ ğŸ› ï¸ utils/                          # Professional utilities
â”‚   â”œâ”€â”€ mlflow_utils.py               # ğŸ”¬ MLflow integration
â”‚   â”œâ”€â”€ logger.py                     # Training logging
â”‚   â””â”€â”€ plotter.py                    # Visualization tools
â”‚
â””â”€â”€ ğŸ“‚ Auto-Generated Outputs          # ğŸ¤– Automated organization
    â”œâ”€â”€ outputs/                      # Hydra experiment outputs
    â”‚   â””â”€â”€ movie_recommendation/     # Timestamped runs
    â”œâ”€â”€ mlruns/                       # ğŸ”¬ MLflow tracking database
    â”‚   â”œâ”€â”€ experiments/              # Organized experiments
    â”‚   â””â”€â”€ models/                   # Model registry
    â””â”€â”€ data/raw/                     # MovieLens dataset
```

**ğŸ¯ Each directory serves a specific purpose in the tech stack integration**

## ğŸ”¬ **MLflow Integration - Zero-Configuration Tracking**

**Every experiment is automatically tracked - no extra code required!**

```bash
# ğŸ¯ Run experiment - MLflow handles everything
python train_hydra.py model=hybrid train.epochs=10

# ğŸ“Š View results dashboard
python check_mlflow.py

# ğŸŒ Launch interactive UI 
python -m mlflow ui --port 5000    # â†’ http://127.0.0.1:5000
```

### **ğŸ¤– Automatic Tracking Features**
- âœ… **Hyperparameters**: All Hydra configs automatically logged
- âœ… **Metrics**: RMSE, MAE, Precision@K tracked every epoch  
- âœ… **Models**: Best checkpoints saved with metadata
- âœ… **Artifacts**: Training plots, config files, logs
- âœ… **System Info**: Hardware specs, execution time, Git hash

### **ğŸ† Production Model Loading**
```python
from utils.mlflow_utils import MLflowModelSelector

# One-liner to get best model
selector = MLflowModelSelector()
best_model, run_id = selector.load_best_model('val_rmse')
```

## ğŸ¯ **Optuna Hyperparameter Optimization**

**Intelligent parameter search integrated with Hydra and MLflow:**

```bash
# ğŸš€ Quick optimization test (6 trials)
python train_hydra.py --config-name=optuna_test -m

# ğŸ“Š All trials automatically tracked in MLflow
python check_mlflow.py  # View results
```

### **ğŸ§  Smart Search vs Grid Search**
| Approach | Trials | Time | Result Quality |
|----------|--------|------|----------------|
| **ğŸ¯ Optuna** | 20 trials | âš¡ Efficient | ğŸ† Optimal |
| ğŸ“Š Grid Search | 64 trials | â° Exhaustive | âœ… Complete |
| ğŸ² Random | 20 trials | âš¡ Fast | ğŸ“Š Variable |

### **ğŸ”§ Optuna Configuration**
```yaml
# conf/hydra/sweeper/optuna_quick.yaml
search_space:
  train.learning_rate: 0.001,0.005,0.01,0.05
  train.batch_size: 256,512  
  model.embedding_dim: 32,50,64
```

**ğŸ”„ Optuna automatically explores the most promising parameter combinations**

## âš™ï¸ **Hydra Configuration Magic**

**Change everything without touching code - just modify YAML files or command line**

**Change everything without touching code - just modify YAML files or command line**

### **ğŸ“ YAML Configuration Files**
```yaml
# conf/model/hybrid.yaml
name: hybrid
embedding_dim: 50
collaborative_weight: 0.7
fusion_dims: [256, 128]

# conf/train/production.yaml
epochs: 30
batch_size: 256
learning_rate: 0.005
patience: 8
```

### **ğŸš€ Command Line Overrides**
```bash
# Override any parameter instantly
python train_hydra.py model=hybrid train.learning_rate=0.001
python train_hydra.py train=production model.embedding_dim=64
```

## ğŸ§  **PyTorch Model Architectures**

**Four production-ready models demonstrating different ML approaches:**

| Model | Approach | Architecture | Use Case |
|-------|----------|--------------|----------|
| ğŸ¤ **Collaborative** | Matrix Factorization | User/Movie embeddings | High accuracy, cold start issues |
| ğŸ¬ **Content-Based** | Feature Engineering | Genre + Neural Networks | Interpretable, works with new movies |
| ğŸ”€ **Hybrid** | Multi-Modal Fusion | Combined approach | **Best performance** (RMSE: 0.3239) |  
| ğŸ§  **Deep CF** | Deep Learning | Multi-layer neural nets | Complex patterns, requires more data |

### **ğŸ—ï¸ Professional PyTorch Implementation**
```python
# Extensible base class
class BaseModel(nn.Module, ABC):
    @abstractmethod
    def forward(self, user_ids, movie_ids): pass
    @abstractmethod  
    def predict(self, user_ids, movie_ids): pass
    
# Easy model switching via Hydra config
model = create_model(cfg, n_users, n_movies)  # cfg.model.type determines architecture
```

## ğŸ“Š **Comprehensive ML Components**

**Professional PyTorch ecosystem with modular components:**

### **ğŸ¯ Loss Functions**
```python
from losses import RecommenderLoss, BPRLoss
criterion = RecommenderLoss(loss_type='bce')  # Configurable via Hydra
```

### **ğŸ“ˆ Evaluation Metrics** 
```python  
from metrics import RecommenderMetrics
metrics = ['rmse', 'mae', 'precision@10', 'recall@10', 'ndcg@10']  # Auto-tracked
```

### **âš¡ Optimizers**
```python
from optimizers import RecommenderOptimizer
optimizer = RecommenderOptimizer("adam").create_optimizer(model, lr=0.001)
```

**ğŸ”„ All components follow the same design pattern and integrate seamlessly with Hydra configuration**

## ï¿½ï¸ Utilities

### **Logger**
Professional logging with training tracking:
```python
from utils import Logger

logger = Logger('logs/train.log')
logger.log_training_start(epochs=20, batch_size=256, lr=0.01)
logger.log_epoch(epoch=0, total_epochs=20, train_loss=0.5, val_loss=0.4, epoch_time=30.2)
```

### **Timer**
Performance timing:
```python
from utils import Timer

timer = Timer()
timer.start()
# ... training code ...
epoch_time = timer.stop()
```

### **Plotter**
Visualization tools:
```python
from utils import TrainingPlotter

plotter = TrainingPlotter()
plotter.plot_losses(train_losses, val_losses, save_path='results/plots/losses.png')
```

## ğŸš€ Training Options

### **Traditional Command Line Arguments**
```bash
python train.py \
    --epochs 50 \
    --batch-size 512 \
    --lr 0.001 \
    --embedding-dim 100 \
    --dropout 0.3 \
    --save-path results/my_model.pth
```

### **Hydra-Based Configuration System**
```bash
# Single model training
python train_hydra.py model=deep_cf train.epochs=50 train.batch_size=512

# Multiple model comparison
python train_hydra.py -m model=collaborative,content_based,hybrid,deep_cf

# Hyperparameter optimization
python train_hydra.py -m train.learning_rate=0.001,0.01,0.1 model.embedding_dim=32,64,128

# Use preset configurations
python train_hydra.py train=production model=hybrid
```

## ğŸ“Š **Data Pipeline Integration**

**Professional PyTorch data handling with MovieLens dataset:**

- ğŸ¬ **MovieLens Dataset**: 100K+ ratings, 9K+ movies with genre metadata
- ğŸ”„ **PyTorch DataLoader**: Efficient batching with negative sampling  
- âš¡ **GPU Acceleration**: CUDA-optimized data loading and model training
- ğŸ›¡ï¸ **Error Handling**: Robust data validation with clear error messages

## ğŸ¯ **Why This Tech Stack?**

**This project demonstrates professional ML engineering with integrated modern tools:**

| Component | Purpose | Benefit |
|-----------|---------|---------|
| ğŸ”¥ **PyTorch** | Model Training | Production-ready deep learning framework |
| âš™ï¸ **Hydra** | Configuration | Experiment reproducibility without code changes |
| ğŸ”¬ **MLflow** | Tracking | Automated experiment logging and model registry |
| ğŸ **Professional OOP** | Architecture | Maintainable, extensible, testable codebase |

### **ğŸš€ Ready to Extend**
```python
# Adding new models, losses, metrics follows the same pattern
class NewModel(BaseModel):  # Inherit from base
    def forward(self): pass  # Implement required methods
    
# Register in configuration 
# conf/model/new_model.yaml â† Add config
# python train_hydra.py model=new_model â† Use immediately
```

## ğŸš€ **Get Started Now**

**Experience the complete tech stack integration in 4 simple commands:**

```bash
# 1ï¸âƒ£ Clone and setup
git clone https://github.com/MrEleden/reco_app.git && cd reco_app && pip install -r requirements.txt

# 2ï¸âƒ£ Basic PyTorch + Hydra + MLflow integration
python train_hydra.py model=hybrid train.epochs=5

# 3ï¸âƒ£ Multi-model comparison with automatic tracking
python train_hydra.py -m model=collaborative,hybrid train.epochs=5

# 4ï¸âƒ£ Intelligent hyperparameter optimization with Optuna
python train_hydra.py --config-name=optuna_test -m

# ğŸ“Š View comprehensive results dashboard  
python check_mlflow.py && python -m mlflow ui --port 5000
```

### **ğŸ¯ Complete Tech Stack Examples**

**ğŸ”¥ PyTorch Models**: Switch architectures via configuration
```bash
python train_hydra.py model=collaborative    # Matrix factorization
python train_hydra.py model=hybrid          # Multi-modal fusion
python train_hydra.py model=deep_cf         # Deep learning
```

**âš™ï¸ Hydra Configuration**: No code changes needed
```bash  
python train_hydra.py train=fast            # Quick 10-epoch training
python train_hydra.py train=production      # Full 30-epoch training
python train_hydra.py optimizer=sgd         # Change optimizer
```

**ğŸ¯ Optuna Optimization**: Smart hyperparameter search
```bash
python train_hydra.py --config-name=optuna_test -m     # Quick optimization
python train_hydra.py --config-name=optuna_demo -m     # Full demo
```

**ğŸ”¬ MLflow Tracking**: Every experiment automatically logged
```bash
python check_mlflow.py                      # Command-line results
python -m mlflow ui --port 5000            # Web dashboard
```

## ğŸ”§ **Quick Troubleshooting**

```bash
# âœ… Test GPU availability
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"

# âœ… Test all imports
python -c "from models import *; from data import *; print('âœ… All systems ready!')"
```

## ğŸ¯ **Project Goals Achieved**

âœ… **Simple**: One command runs complete ML pipeline  
âœ… **Consistent**: All components follow same design patterns  
âœ… **Professional**: Production-ready code with proper error handling  
âœ… **Integrated**: PyTorch + Hydra + MLflow + Optuna work seamlessly together  
âœ… **Intelligent**: Automated hyperparameter optimization with smart search
âœ… **Extensible**: Easy to add new models, losses, metrics, optimizers  

---

**ğŸ¬ Complete Modern ML Stack: PyTorch + Hydra + MLflow + Optuna** âš¡

*The perfect foundation for your next ML project!* ğŸš€
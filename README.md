# Movie Recommendation System# ğŸ¬ Movie Recommendation System# ğŸ¬ Movie Recommendation System Tech Stack Integration



A PyTorch-based movie recommendation system that demonstrates scaling from scratch implementation to production-ready ML pipeline.



## Project Evolution**From PyTorch Scratch to Production-Ready ML Pipeline**A **production-ready movie recommendation system** demonstrating how modern ML/AI technologies work together seamlessly. This project showcases the integration of **PyTorch**, **Hydra**, **MLflow**, **Optuna**, and professional software architecture.



This project shows the progression from basic PyTorch to a complete ML system:



1. **From Scratch Implementation** (`train.py`)## ğŸ¯ **Why This Project?**## ğŸ¯ **Tech Stack Showcase**

   - Pure PyTorch movie recommendation models

   - Basic training loop and evaluation

   - Simple data handling

Built to demonstrate the **evolution of ML development**:This project demonstrates **complete integration** of modern ML technologies:

2. **Scaled Production System** (`train_hydra.py`) 

   - Hydra configuration management- ğŸ”¥ **From Scratch**: Pure PyTorch implementation (`train.py`)

   - MLflow experiment tracking

   - Optuna hyperparameter optimization- ğŸ—ï¸ **Modular Design**: Reusable components for scripts and notebooks  | Technology | Purpose | Integration Point |

   - Professional logging and model persistence

- ğŸš€ **Production Stack**: Integrated PyTorch + Hydra + MLflow + Optuna (`train_hydra.py`)|------------|---------|-------------------|

## What I Built

| **ğŸ”¥ PyTorch** | Deep Learning Framework | Model architecture, training loops, GPU acceleration |

- **Models**: Matrix Factorization, Neural Collaborative Filtering, DeepFM

- **Configuration**: Hydra-based config system for easy experimentation## âš¡ **One Command, Complete Pipeline**| **âš™ï¸ Hydra** | Configuration Management | Experiment configs, multirun sweeps, parameter overrides |

- **Tracking**: MLflow integration for experiment management

- **Optimization**: Optuna for automatic hyperparameter tuning| **ğŸ”¬ MLflow** | Experiment Tracking | Automatic logging, model registry, performance comparison |

- **Demo**: Streamlit app for interactive recommendations

```bash| **ğŸ¯ Optuna** | Hyperparameter Optimization | Intelligent parameter search with Hydra integration |

## Usage

# Basic PyTorch training| **ğŸ Python OOP** | Code Architecture | Modular design, abstract classes, professional structure |

Basic PyTorch training:

```bashpython train.py| **ğŸ“Š Scientific Stack** | Data Processing | NumPy, Pandas for data manipulation and analysis |

python train.py

```



Production pipeline with optimization:# Production-ready with hyperparameter optimization### **ğŸš€ Key Integration Features**

```bash

python train_hydra.py -m model=collaborative,hybrid,deep_cfpython train_hydra.py -m model=collaborative,hybrid,deep_cf train=production hydra/sweeper=optuna_production- **One Command, Full Pipeline**: `python train_hydra.py -m model=collaborative,hybrid,deep_cf`

```

```- **Intelligent Optimization**: `python train_hydra.py --config-name=optuna_test -m` 

Launch demo app:

```bash- **Automatic Tracking**: Every experiment logged to MLflow with zero extra code

streamlit run app.py

```## ğŸ† **Tech Stack Evolution**- **Configuration Magic**: Change models, optimizers, hyperparameters via YAML configs



## Key Technologies- **Smart HPO**: Optuna finds optimal hyperparameters automatically



- PyTorch: Deep learning framework| Approach | Command | Features |- **Production Ready**: Professional error handling, logging, and model persistence

- Hydra: Configuration management

- MLflow: Experiment tracking|----------|---------|----------|

- Optuna: Hyperparameter optimization

- Streamlit: Interactive web app| **ğŸ”¥ Basic** | `python train.py` | Pure PyTorch, manual tuning |## ğŸ† **Live Performance Dashboard**

| **âš™ï¸ Structured** | `python train_hydra.py model=hybrid` | Configuration management |

| **ğŸš€ Production** | `python train_hydra.py -m [models] hydra/sweeper=optuna` | Auto-optimization + tracking |**Current Best Model**: Hybrid Architecture with **RMSE: 0.3239** and **85.21% Accuracy**



## ğŸ“Š **Current Best Model**| Model | RMSE â¬‡ï¸ | Accuracy â¬†ï¸ | Status |

|-------|---------|-------------|---------|

**Hybrid Architecture**: 85.41% accuracy, RMSE 0.3232  | ğŸ¥‡ **Hybrid** | **0.3239** | **85.21%** | âœ… Production Ready |

*Automatically discovered via Optuna optimization*| ğŸ¥ˆ Collaborative | 0.3451 | 83.65% | âœ… Baseline |

| ğŸ¥‰ Content-Based | 0.3820 | 82.25% | âœ… Specialized |

## ğŸ® **Interactive Demo**

*Real-time results from MLflow tracking - View full dashboard: http://127.0.0.1:5000*

```bash

streamlit run app.py## ğŸš€ **Get Started Now**

# â†’ Live model selection, real-time recommendations, MLflow integration

```**Experience the complete tech stack integration in 4 simple commands:**



## ğŸ—ï¸ **Architecture Highlights**```bash

# 1ï¸âƒ£ Clone and setup

- **ğŸ“¦ Modular Components**: Reusable across scripts and notebooksgit clone https://github.com/MrEleden/reco_app.git && cd reco_app && pip install -r requirements.txt

- **âš™ï¸ Configuration-Driven**: Change models/parameters without code changes  

- **ğŸ“Š Auto-Tracking**: Every experiment logged to MLflow# 2ï¸âƒ£ Basic PyTorch + Hydra + MLflow integration

- **ğŸ¯ Smart Optimization**: Optuna finds best hyperparameters automaticallypython train_hydra.py model=hybrid train.epochs=5

- **ğŸŒ Deploy-Ready**: Streamlit app ready for Hugging Face Spaces

# 3ï¸âƒ£ Multi-model comparison with automatic tracking

## ğŸš€ **Quick Start**python train_hydra.py -m model=collaborative,hybrid train.epochs=5



```bash# 4ï¸âƒ£ Production-ready hyperparameter optimization with Optuna

# Clone and setuppython train_hydra.py -m model=collaborative,content_based,hybrid,deep_cf train=production hydra/sweeper=optuna_production

git clone [repo-url] && cd reco_app

pip install -r requirements.txt# ğŸ“Š View comprehensive results dashboard  

python check_mlflow.py && python -m mlflow ui --port 5000

# Try different approaches```

python train.py                           # Basic PyTorch

python train_hydra.py model=hybrid        # Structured training  ### **ğŸ¯ Complete Tech Stack Examples**

python train_hydra.py -m model=all        # Multi-model comparison

streamlit run app.py                      # Interactive demo**ğŸ”¥ PyTorch Models**: Switch architectures via configuration

``````bash

python train_hydra.py model=collaborative    # Matrix factorization

---python train_hydra.py model=hybrid          # Multi-modal fusion

python train_hydra.py model=deep_cf         # Deep learning

**ğŸ¯ From scratch PyTorch to production ML pipeline in simple commands**```

**âš™ï¸ Hydra Configuration**: No code changes needed
```bash  
python train_hydra.py train=fast            # Quick 10-epoch training
python train_hydra.py train=production      # Full 100-epoch training
python train_hydra.py optimizer=sgd         # Change optimizer
```

**ğŸ¯ Optuna Optimization**: Smart hyperparameter search
```bash
# Quick test optimization (6 trials)
python train_hydra.py --config-name=optuna_test -m

# Production optimization (100 trials, 4 parallel jobs)
python train_hydra.py -m model=collaborative,content_based,hybrid,deep_cf train=production hydra/sweeper=optuna_production
```

**ğŸ”¬ MLflow Tracking**: Every experiment automatically logged
```bash
python check_mlflow.py                      # Command-line results
python -m mlflow ui --port 5000            # Web dashboard
```

## ğŸ”¬ **MLflow Integration - Zero-Configuration Tracking**

**Every experiment is automatically tracked - no extra code required!**

```bash
# ğŸ¯ Run experiment - MLflow handles everything
python train_hydra.py model=hybrid train.epochs=10

# ğŸ“Š View results dashboard
python check_mlflow.py

# ğŸŒ Launch interactive UI 
python -m mlflow ui --port 5000    # â†’ http://127.0.0.1:5000
```

### **ğŸ¤– Automatic Tracking Features**
- âœ… **Hyperparameters**: All Hydra configs automatically logged
- âœ… **Metrics**: RMSE, MAE, Precision@K tracked every epoch  
- âœ… **Models**: Best checkpoints saved with metadata
- âœ… **Artifacts**: Training plots, config files, logs
- âœ… **System Info**: Hardware specs, execution time, Git hash

### **ğŸ† Production Model Loading**
```python
from utils.mlflow_utils import MLflowModelSelector

# One-liner to get best model
selector = MLflowModelSelector()
best_model, run_id = selector.load_best_model('val_rmse')
```

## ğŸ¯ **Optuna Hyperparameter Optimization**

**Production-ready intelligent parameter search integrated with Hydra and MLflow:**

### **ğŸš€ Quick Start Examples**
```bash
# ğŸ§ª Quick optimization test (6 trials)
python train_hydra.py --config-name=optuna_test -m

# ğŸ­ Production multi-model optimization (100 trials, 4 parallel jobs)
python train_hydra.py -m model=collaborative,content_based,hybrid,deep_cf train=production hydra/sweeper=optuna_production

# ğŸ“Š All trials automatically tracked in MLflow
python check_mlflow.py  # View results
```

### **ğŸ† Production Configuration Features**
- **100 Intelligent Trials**: TPE sampler with smart parameter exploration
- **4 Parallel Jobs**: Faster optimization using multiple CPU cores
- **Multi-Model Search**: Optimizes all 4 models simultaneously
- **Automatic MLflow Tracking**: Every trial logged with hyperparameters and results
- **Production Training**: Uses full 100-epoch training with early stopping

### **ğŸ”§ Optimized Parameters**
| Parameter | Search Space | Strategy |
|-----------|--------------|----------|
| `model` | collaborative, content_based, hybrid, deep_cf | Intelligent model selection |
| `learning_rate` | 0.0001 â†’ 0.05 | Log-scale discrete choices |
| `batch_size` | 128, 256, 512, 1024 | Memory-efficient powers of 2 |
| `dropout` | 0.1 â†’ 0.5 | Regularization optimization |
| `weight_decay` | 0.0001 â†’ 0.01 | Log-scale regularization |
| `patience` | 5 â†’ 15 epochs | Early stopping optimization |

### **ğŸ§  Smart Search vs Grid Search**
| Approach | Trials | Time | Result Quality | Parallel |
|----------|--------|------|----------------|----------|
| **ğŸ¯ Optuna Production** | 100 trials | âš¡ Efficient | ğŸ† Optimal | âœ… 4 jobs |
| **ğŸ§ª Optuna Quick** | 6 trials | âš¡ Ultra-fast | ğŸ“Š Good | âŒ 1 job |
| ğŸ“Š Grid Search | 1000+ trials | â° Exhaustive | âœ… Complete | âŒ Sequential |
| ğŸ² Random | 100 trials | âš¡ Fast | ğŸ“Š Variable | âŒ Sequential |

**ğŸ”„ Optuna's TPE sampler automatically explores the most promising parameter combinations based on previous trial results**

## âš™ï¸ **Hydra Configuration Magic**

**Change everything without touching code - just modify YAML files or command line**

### **ğŸ“ YAML Configuration Files**
```yaml
# conf/model/hybrid.yaml
name: hybrid
embedding_dim: 50
collaborative_weight: 0.7
fusion_dims: [256, 128]

# conf/train/production.yaml
epochs: 30
batch_size: 256
learning_rate: 0.005
patience: 8
```

### **ğŸš€ Command Line Overrides**
```bash
# Override any parameter instantly
python train_hydra.py model=hybrid train.learning_rate=0.001
python train_hydra.py train=production model.embedding_dim=64
```

## ğŸ§  **PyTorch Model Architectures**

**Four production-ready models demonstrating different ML approaches:**

| Model | Approach | Architecture | Use Case |
|-------|----------|--------------|----------|
| ğŸ¤ **Collaborative** | Matrix Factorization | User/Movie embeddings | High accuracy, cold start issues |
| ğŸ¬ **Content-Based** | Feature Engineering | Genre + Neural Networks | Interpretable, works with new movies |
| ğŸ”€ **Hybrid** | Multi-Modal Fusion | Combined approach | **Best performance** (RMSE: 0.3239) |  
| ğŸ§  **Deep CF** | Deep Learning | Multi-layer neural nets | Complex patterns, requires more data |

### **ğŸ—ï¸ Professional PyTorch Implementation**
```python
# Extensible base class
class BaseModel(nn.Module, ABC):
    @abstractmethod
    def forward(self, user_ids, movie_ids): pass
    @abstractmethod  
    def predict(self, user_ids, movie_ids): pass
    
# Easy model switching via Hydra config
model = create_model(cfg, n_users, n_movies)  # cfg.model.type determines architecture
```

## ğŸ“Š **Comprehensive ML Components**

**Professional PyTorch ecosystem with modular components:**

### **ğŸ¯ Loss Functions**
```python
from losses import RecommenderLoss, BPRLoss
criterion = RecommenderLoss(loss_type='bce')  # Configurable via Hydra
```

### **ğŸ“ˆ Evaluation Metrics** 
```python  
from metrics import RecommenderMetrics
metrics = ['rmse', 'mae', 'precision@10', 'recall@10', 'ndcg@10']  # Auto-tracked
```

### **âš¡ Optimizers**
```python
from optimizers import RecommenderOptimizer
optimizer = RecommenderOptimizer("adam").create_optimizer(model, lr=0.001)
```

**ğŸ”„ All components follow the same design pattern and integrate seamlessly with Hydra configuration**

## ğŸ“ **Tech Stack Architecture**

**Clean, Professional Structure Showcasing Modern ML Engineering**

```
reco_app/
â”œâ”€â”€ ğŸš€ train_hydra.py                 # Main entry point - Hydra + PyTorch + MLflow
â”œâ”€â”€ ğŸ”¬ check_mlflow.py                # MLflow results dashboard
â”œâ”€â”€ ğŸ“‹ requirements.txt               # Tech stack dependencies
â”‚
â”œâ”€â”€ âš™ï¸ conf/                           # ğŸ¯ HYDRA Configuration Hub
â”‚   â”œâ”€â”€ config.yaml                   # Main orchestration config
â”‚   â”œâ”€â”€ model/                        # ğŸ§  Model architecture configs
â”‚   â”‚   â”œâ”€â”€ collaborative.yaml        # Matrix factorization
â”‚   â”‚   â”œâ”€â”€ hybrid.yaml               # Multi-modal approach
â”‚   â”‚   â””â”€â”€ deep_cf.yaml              # Deep learning model
â”‚   â”œâ”€â”€ train/                        # ğŸš€ Training configurations
â”‚   â”‚   â”œâ”€â”€ fast.yaml                 # Quick experiments (10 epochs)
â”‚   â”‚   â””â”€â”€ production.yaml           # Full training (100 epochs)
â”‚   â”œâ”€â”€ optimizer/                    # âš¡ Optimizer settings
â”‚   â”‚   â”œâ”€â”€ adam.yaml                 # Adaptive learning
â”‚   â”‚   â””â”€â”€ sgd.yaml                  # Stochastic gradient descent
â”‚   â””â”€â”€ hydra/sweeper/                # ğŸ¯ Optuna HPO configurations
â”‚       â”œâ”€â”€ optuna_quick.yaml         # Quick HPO (6 trials)
â”‚       â”œâ”€â”€ optuna_comprehensive.yaml # Standard HPO (50 trials)
â”‚       â””â”€â”€ optuna_production.yaml    # Production HPO (100 trials, 4 jobs)
â”‚
â”œâ”€â”€ ğŸ§  models/                         # ğŸ”¥ PYTORCH Model Architectures
â”‚   â”œâ”€â”€ collaborative_filtering.py    # Matrix factorization
â”‚   â”œâ”€â”€ content_based_model.py        # Genre-based recommendations  
â”‚   â”œâ”€â”€ hybrid_model.py               # Combined approach
â”‚   â””â”€â”€ deep_collaborative_filtering.py # Deep neural networks
â”‚
â”œâ”€â”€ ğŸ“Š data/                           # Data processing pipeline
â”‚   â”œâ”€â”€ dataset.py                    # PyTorch datasets
â”‚   â””â”€â”€ dataloader.py                 # MovieLens data loader
â”‚
â”œâ”€â”€ ğŸ¯ losses/ & ğŸ“ˆ metrics/           # Training components
â”‚   â”œâ”€â”€ loss.py                       # BCE, MSE, ranking losses
â”‚   â””â”€â”€ metric.py                     # RMSE, MAE, Precision@K
â”‚
â”œâ”€â”€ ğŸ› ï¸ utils/                          # Professional utilities
â”‚   â”œâ”€â”€ mlflow_utils.py               # ğŸ”¬ MLflow integration
â”‚   â”œâ”€â”€ logger.py                     # Training logging
â”‚   â””â”€â”€ plotter.py                    # Visualization tools
â”‚
â””â”€â”€ ğŸ“‚ Auto-Generated Outputs          # ğŸ¤– Automated organization
    â”œâ”€â”€ outputs/                      # Hydra experiment outputs
    â”‚   â””â”€â”€ movie_recommendation/     # Timestamped runs
    â”œâ”€â”€ mlruns/                       # ğŸ”¬ MLflow tracking database
    â”‚   â”œâ”€â”€ experiments/              # Organized experiments
    â”‚   â””â”€â”€ models/                   # Model registry
    â””â”€â”€ data/raw/                     # MovieLens dataset
```

**ğŸ¯ Each directory serves a specific purpose in the tech stack integration**

## ğŸ“Š **Data Pipeline Integration**

**Professional PyTorch data handling with MovieLens dataset:**

- ğŸ¬ **MovieLens Dataset**: 100K+ ratings, 9K+ movies with genre metadata
- ğŸ”„ **PyTorch DataLoader**: Efficient batching with negative sampling  
- âš¡ **GPU Acceleration**: CUDA-optimized data loading and model training
- ğŸ›¡ï¸ **Error Handling**: Robust data validation with clear error messages

## ğŸ¯ **Why This Tech Stack?**

**This project demonstrates professional ML engineering with integrated modern tools:**

| Component | Purpose | Benefit |
|-----------|---------|---------|
| ğŸ”¥ **PyTorch** | Model Training | Production-ready deep learning framework |
| âš™ï¸ **Hydra** | Configuration | Experiment reproducibility without code changes |
| ğŸ”¬ **MLflow** | Tracking | Automated experiment logging and model registry |
| ğŸ¯ **Optuna** | Optimization | Intelligent hyperparameter search |

### **ğŸš€ Ready to Extend**
```python
# Adding new models, losses, metrics follows the same pattern
class NewModel(BaseModel):  # Inherit from base
    def forward(self): pass  # Implement required methods
    
# Register in configuration 
# conf/model/new_model.yaml â† Add config
# python train_hydra.py model=new_model â† Use immediately
```

## ğŸ”§ **Quick Troubleshooting**

```bash
# âœ… Test GPU availability
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"

# âœ… Test all imports
python -c "from models import *; from data import *; print('âœ… All systems ready!')"
```

## ğŸ¯ **Project Goals Achieved**

âœ… **Simple**: One command runs complete ML pipeline  
âœ… **Consistent**: All components follow same design patterns  
âœ… **Professional**: Production-ready code with proper error handling  
âœ… **Integrated**: PyTorch + Hydra + MLflow + Optuna work seamlessly together  
âœ… **Intelligent**: Automated hyperparameter optimization with smart search
âœ… **Extensible**: Easy to add new models, losses, metrics, optimizers  

---

**ğŸ¬ Complete Modern ML Stack: PyTorch + Hydra + MLflow + Optuna** âš¡

*The perfect foundation for your next ML project!* ğŸš€